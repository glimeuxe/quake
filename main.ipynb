{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, joblib, json, os, time, torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from datetime import datetime, timedelta\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "BASE_PATH = os.path.expanduser(\"~/Code/Quake\")\n",
    "DATA_PATH, SAVES_PATH = os.path.join(BASE_PATH, \"data\"), os.path.join(BASE_PATH, \"saves\")\n",
    "DATABASE_FILE, PROCESSED_DATABASE_FILE, METRICS_FILE = os.path.join(DATA_PATH, \"database.csv\"), os.path.join(DATA_PATH, \"processed_database.npz\"), os.path.join(SAVES_PATH, \"all.metrics\")\n",
    "\n",
    "R, I = 100, 2592000\n",
    "\n",
    "def check_repository():\n",
    "\tos.makedirs(DATA_PATH, exist_ok=True)\n",
    "\tos.makedirs(SAVES_PATH, exist_ok=True)\n",
    "\tif not os.path.isfile(DATABASE_FILE):\n",
    "\t\traise Exception()\n",
    "\tif not os.path.isfile(METRICS_FILE) or os.path.getsize(METRICS_FILE) == 0:\n",
    "\t\twith open(METRICS_FILE, \"w\") as f:\n",
    "\t\t\tf.write(\"[]\")\n",
    "\tprint(\"Checked repository\")\n",
    "\n",
    "class EarthquakeDataset(Dataset):\n",
    "\tdef __init__(self, X, y):\n",
    "\t\tself.X, self.y = torch.tensor(X, dtype=torch.float32, device=DEVICE), torch.tensor(y, dtype=torch.float32, device=DEVICE)\n",
    "\tdef __getitem__(self, i):\n",
    "\t\treturn self.X[i], self.y[i]\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.X)\n",
    "\n",
    "def generate_labels(df):\n",
    "\ttimestamps, latitudes, longitudes = df[\"Timestamp\"].values, df[\"Latitude\"].values, df[\"Longitude\"].values\n",
    "\tm = len(timestamps)\n",
    "\ty = np.zeros(m, dtype=np.float32)\n",
    "\tfor i in range(m):\n",
    "\t\tevent_time, event_latitude, event_longitude = timestamps[i], latitudes[i], longitudes[i]\n",
    "\t\tin_window = (timestamps > event_time) & (timestamps <= event_time + I)\n",
    "\t\tfor j in np.where(in_window)[0]:\n",
    "\t\t\tif geodesic((event_latitude, event_longitude), (latitudes[j], longitudes[j])).km <= R:\n",
    "\t\t\t\ty[i] = 1\n",
    "\t\t\t\tbreak\n",
    "\treturn y\n",
    "\n",
    "def process_dataset():\n",
    "\tif os.path.exists(PROCESSED_DATABASE_FILE):\n",
    "\t\tX, y = np.load(PROCESSED_DATABASE_FILE)[\"X\"], np.load(PROCESSED_DATABASE_FILE)[\"y\"]\n",
    "\t\t# print(pd.DataFrame(X, columns=[\"Timestamp\", \"Latitude\", \"Longitude\", \"Depth\", \"Magnitude\"]))\n",
    "\t\treturn X, y\n",
    "\n",
    "\tdf = pd.read_csv(DATABASE_FILE, sep=\",\", engine=\"python\")\n",
    "\n",
    "\t# Select \"Earthquake\" rows\n",
    "\tdf = df[df[\"Type\"] == \"Earthquake\"]\n",
    "\n",
    "\t# Combine \"Date\" and \"Time\" columns to \"Timestamp\" column in Datetime format\n",
    "\tdf[\"Timestamp\"] = pd.to_datetime(\n",
    "\t\tdf[\"Date\"] + \" \" + df[\"Time\"],\n",
    "\t\tformat=\"%m/%d/%Y %H:%M:%S\",\n",
    "\t\terrors=\"coerce\"\n",
    "\t)\n",
    "\n",
    "\t# Drop non-standard \"Timestamp\" columns\n",
    "\tdf = df.dropna(subset=[\"Timestamp\"])\n",
    "\n",
    "\t# Convert \"Timestamp\" column from Datetime format to UNIX format\n",
    "\tdf[\"Timestamp\"] = df[\"Timestamp\"].view(\"int64\") // 10**9\n",
    "\n",
    "\t# Select and sort \"Timestamp\", \"Latitude\", \"Longitude\", \"Depth\", and \"Magnitude\" columns\n",
    "\tdf = df[[\"Timestamp\", \"Latitude\", \"Longitude\", \"Depth\", \"Magnitude\"]].sort_values(\"Timestamp\")\n",
    "\n",
    "\tX = df.values\n",
    "\ty = generate_labels(df)\n",
    "\n",
    "\tnp.savez(PROCESSED_DATABASE_FILE, X=X, y=y)\n",
    "\treturn X, y\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "\tdef __init__(self, id):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.id = id\n",
    "\tdef forward(self, x):\n",
    "\t\traise NotImplementedError()\n",
    "\n",
    "class MLPModel(BaseModel):\n",
    "\tdef __init__(self, input_dimensions, hidden_dimensions, layers, dropout_rate, is_batch_normalised):\n",
    "\t\tsuper().__init__(f\"MLP({input_dimensions}-{hidden_dimensions}-{layers}-{dropout_rate}-{is_batch_normalised})\")\n",
    "\n",
    "\t\tlayer_list = []\n",
    "\t\tfor i in range(layers):\n",
    "\t\t\tin_dim = input_dimensions if i == 0 else hidden_dimensions\n",
    "\t\t\tlayer_list.append(nn.Linear(in_dim, hidden_dimensions))\n",
    "\t\t\tif is_batch_normalised:\n",
    "\t\t\t\tlayer_list.append(nn.BatchNorm1d(hidden_dimensions))\n",
    "\t\t\tlayer_list.append(nn.ReLU())\n",
    "\t\t\tlayer_list.append(nn.Dropout(dropout_rate))\n",
    "\t\tlayer_list.append(nn.Linear(hidden_dimensions, 1))\n",
    "\n",
    "\t\tself.network = nn.Sequential(*layer_list)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn torch.sigmoid(self.network(x)).squeeze(-1)\n",
    "\n",
    "class TranEncDecModel(BaseModel):\n",
    "\tdef __init__(self, input_dimensions, hidden_dimensions, layers, heads, dropout_rate, is_batch_normalised):\n",
    "\t\tsuper().__init__(f\"TranEncDec({input_dimensions}-{hidden_dimensions}-{layers}-{heads}-{dropout_rate}-{is_batch_normalised})\")\n",
    "\n",
    "\t\tencoder_layer = nn.TransformerEncoderLayer(\n",
    "\t\t\td_model=input_dimensions,\n",
    "\t\t\tnhead=heads,\n",
    "\t\t\tdim_feedforward=hidden_dimensions,\n",
    "\t\t\tdropout=dropout_rate,\n",
    "\t\t\tbatch_first=True\n",
    "\t\t)\n",
    "\t\tself.encoder = nn.TransformerEncoder(encoder_layer, num_layers=layers)\n",
    "\t\tself.decoder = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dimensions, hidden_dimensions),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(hidden_dimensions, 1)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.encoder(x.unsqueeze(1)).squeeze(1)\n",
    "\t\treturn torch.sigmoid(self.decoder(x)).squeeze(-1)\n",
    "\n",
    "class ConvTranEncDecModel(BaseModel):\n",
    "\tdef __init__(self, input_dimensions, conv_filters, kernel_size, hidden_dimensions, layers, heads, dropout_rate, is_batch_normalised):\n",
    "\t\tsuper().__init__(f\"ConvTranEncDec({input_dimensions}-{conv_filters}-{kernel_size}-{hidden_dimensions}-{layers}-{heads}-{dropout_rate}-{is_batch_normalised})\")\n",
    "\n",
    "\t\tself.conv = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(in_channels=1, out_channels=conv_filters, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv1d(in_channels=conv_filters, out_channels=conv_filters, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "\t\t\tnn.ReLU()\n",
    "\t\t)\n",
    "\t\tencoder_layer = nn.TransformerEncoderLayer(\n",
    "\t\t\td_model=conv_filters,\n",
    "\t\t\tnhead=heads,\n",
    "\t\t\tdim_feedforward=hidden_dimensions,\n",
    "\t\t\tdropout=dropout_rate,\n",
    "\t\t\tbatch_first=True\n",
    "\t\t)\n",
    "\t\tself.encoder = nn.TransformerEncoder(encoder_layer, num_layers=layers)\n",
    "\t\tself.decoder = nn.Sequential(\n",
    "\t\t\tnn.Linear(conv_filters, hidden_dimensions),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(hidden_dimensions, 1)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x.unsqueeze(1)\n",
    "\t\tx = self.conv(x).transpose(1, 2)\n",
    "\t\tx = self.encoder(x).mean(dim=1)\n",
    "\t\treturn torch.sigmoid(self.decoder(x)).squeeze(-1)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "\t\"MLP\": MLPModel,\n",
    "\t\"TranEncDec\": TranEncDecModel,\n",
    "\t\"ConvTranEncDec\": ConvTranEncDecModel\n",
    "}\n",
    "\n",
    "def instantiate_model(model_type, **kwargs):\n",
    "\treturn MODEL_CLASSES[model_type](**kwargs).to(DEVICE)\n",
    "\n",
    "def save_model_weights_and_losses(model, epoch, train_loss, val_loss):\n",
    "\tmodel_folder = os.path.join(SAVES_PATH, model.id)\n",
    "\n",
    "\t# Make model folder if and only if it does not exist\n",
    "\tos.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "\t# Save model weights at epoch number to model folder\n",
    "\ttorch.save(model.state_dict(), os.path.join(model_folder, f\"epoch-{epoch}.pth\"))\n",
    "\n",
    "\t# Save model losses at epoch number to model folder\n",
    "\twith open(os.path.join(model_folder, f\"epoch-{epoch}.losses\"), \"w\") as f:\n",
    "\t\tjson.dump({\"train\": train_loss, \"val\": val_loss}, f)\n",
    "\n",
    "def load_model_weights(model, epoch):\n",
    "\tmodel_folder = os.path.join(SAVES_PATH, model.id)\n",
    "\tmodel_weights = sorted(glob.glob(os.path.join(model_folder, \"*.pth\")), key=os.path.getmtime)\n",
    "\n",
    "\tif not model_weights:\n",
    "\t\treturn 1\n",
    "\n",
    "\tif epoch is not None:\n",
    "\t\tloaded_model_weights = os.path.join(model_folder, f\"epoch-{epoch}.pth\")\n",
    "\telse:\n",
    "\t\tloaded_model_weights = model_weights[-1]\n",
    "\t\tepoch = int(os.path.basename(loaded_model_weights).split(\"-\")[1].split(\".\")[0])\n",
    "\tmodel.load_state_dict(torch.load(loaded_model_weights, map_location=DEVICE))\n",
    "\n",
    "\treturn max(epoch + 1, 1)\n",
    "\n",
    "def train_model(model, L_train, L_val, epoch, epochs):\n",
    "\t# Try to load model weights\n",
    "\tinitial_epoch = load_model_weights(model, epoch)\n",
    "\tfinal_epoch = initial_epoch + epochs + 1\n",
    "\n",
    "\toptimizer = torch.optim.Adam(model.parameters())\n",
    "\tcriterion = nn.BCELoss()\n",
    "\n",
    "\tlowest_loss, patience_limit, patience_counter = float(\"inf\"), 5, 0\n",
    "\n",
    "\tfor epoch in range(initial_epoch, final_epoch):\n",
    "\t\tmodel.train()\n",
    "\t\ttrain_loss = 0\n",
    "\t\tfor X_batch, y_batch in L_train:\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\ty_predictions = model(X_batch)\n",
    "\t\t\tloss = criterion(y_predictions, y_batch)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\ttrain_loss += loss.item() * X_batch.size(0)\n",
    "\t\ttrain_loss /= len(L_train.dataset)\n",
    "\n",
    "\t\tmodel.eval()\n",
    "\t\tval_loss = 0\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor X_batch, y_batch in L_val:\n",
    "\t\t\t\ty_predictions = model(X_batch)\n",
    "\t\t\t\tloss = criterion(y_predictions, y_batch)\n",
    "\t\t\t\tval_loss += loss.item() * X_batch.size(0)\n",
    "\t\tval_loss /= len(L_val.dataset)\n",
    "\n",
    "\t\t# Save model weights and losses\n",
    "\t\tsave_model_weights_and_losses(model, epoch, train_loss, val_loss)\n",
    "\n",
    "\t\t# Print model training progress\n",
    "\t\tprint(f\"{model.id} @ epoch {epoch} of {final_epoch}: train_loss = {train_loss:.7f}, val_loss = {val_loss:.7f}\")\n",
    "\n",
    "\t\t# Try early stopping\n",
    "\t\tif val_loss > lowest_loss:\n",
    "\t\t\tpatience_counter += 1\n",
    "\t\t\tif patience_counter >= patience_limit:\n",
    "\t\t\t\tprint(f\"Early stopping at epoch {epoch}\")\n",
    "\t\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\tpatience_counter, lowest_loss = 0, val_loss\n",
    "\n",
    "def evaluate_model(model, L_unseen):\n",
    "\tmodel.eval()\n",
    "\tcorrect, total = 0, 0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor X_batch, y_batch in L_unseen:\n",
    "\t\t\ty_predictions = (model(X_batch) >= 0.5).float()\n",
    "\t\t\tcorrect += (y_predictions == y_batch).sum().item()\n",
    "\t\t\ttotal += y_batch.size(0)\n",
    "\treturn correct / total\n",
    "\n",
    "def train_and_crossvalidate_model(model_specification, folds=1, epoch=None, epochs=1):\n",
    "\tX, y = process_dataset()\n",
    "\n",
    "\tX_seen, X_unseen, y_seen, y_unseen = train_test_split(X, y, test_size=0.2, random_state=37)\n",
    "\tD_unseen = EarthquakeDataset(X_unseen, y_unseen)\n",
    "\tL_unseen = DataLoader(D_unseen, batch_size=32, shuffle=False)\n",
    "\n",
    "\tif folds > 1:\n",
    "\t\tkf = KFold(n_splits=folds, shuffle=True, random_state=37)\n",
    "\t\tscore_list = []\n",
    "\t\tfor fold, (i_train, i_val) in enumerate(kf.split(X_seen)):\n",
    "\t\t\tD_train, D_val = TensorDataset(\n",
    "\t\t\t\ttorch.tensor(X_seen[i_train], dtype=torch.float32, device=DEVICE),\n",
    "\t\t\t\ttorch.tensor(y_seen[i_train], dtype=torch.float32, device=DEVICE)\n",
    "\t\t\t), TensorDataset(\n",
    "\t\t\t\ttorch.tensor(X_seen[i_val], dtype=torch.float32, device=DEVICE),\n",
    "\t\t\t\ttorch.tensor(y_seen[i_val], dtype=torch.float32, device=DEVICE)\n",
    "\t\t\t)\n",
    "\t\t\tL_train, L_val = DataLoader(D_train, batch_size=32, shuffle=True), DataLoader(D_val, batch_size=32, shuffle=False)\n",
    "\n",
    "\t\t\tmodel = instantiate_model(**model_specification)\n",
    "\t\t\ttrain_model(model, L_train, L_val, epoch, epochs)\n",
    "\t\t\tscore_list.append(evaluate_model(model, L_unseen))\n",
    "\t\tscore = sum(score_list) / len(score_list)\n",
    "\telse:\n",
    "\t\tD_seen = EarthquakeDataset(X_seen, y_seen)\n",
    "\t\tL_train, L_val = DataLoader(D_seen, batch_size=32, shuffle=True), DataLoader(D_seen, batch_size=32, shuffle=False)\n",
    "\n",
    "\t\tmodel = instantiate_model(**model_specification)\n",
    "\t\ttrain_model(model, L_train, L_val, epoch, epochs)\n",
    "\t\tscore = evaluate_model(model, L_unseen)\n",
    "\n",
    "\tprint(f\"id: {model.id}, score: {score}\")\n",
    "\twith open(METRICS_FILE, \"r+\") as f:\n",
    "\t\tmetrics = json.load(f)\n",
    "\t\tmetrics.append({\n",
    "\t\t\t\"id\": model.id,\n",
    "\t\t\t\"score\": score,\n",
    "\t\t\t\"folds\": folds,\n",
    "\t\t\t\"time\": time.time()\n",
    "\t\t})\n",
    "\t\tf.seek(0)\n",
    "\t\tjson.dump(metrics, f, indent=2)\n",
    "\t\tf.truncate()\n",
    "\n",
    "check_repository()\n",
    "\n",
    "model_specification_1 = {\n",
    "\t\"model_type\": \"MLP\",\n",
    "\t\"input_dimensions\": 5,\n",
    "\t\"hidden_dimensions\": 128,\n",
    "\t\"layers\": 16,\n",
    "\t\"dropout_rate\": 0.1,\n",
    "\t\"is_batch_normalised\": True\n",
    "}\n",
    "model_specification_2 = {\n",
    "\t\"model_type\": \"TranEncDec\",\n",
    "\t\"input_dimensions\": 5,\n",
    "\t\"hidden_dimensions\": 128,\n",
    "\t\"layers\": 8,\n",
    "\t\"heads\": 5,\n",
    "\t\"dropout_rate\": 0.05,\n",
    "\t\"is_batch_normalised\": True\n",
    "}\n",
    "model_specification_3 = {\n",
    "\t\"model_type\": \"ConvTranEncDec\",\n",
    "\t\"input_dimensions\": 5,\n",
    "\t\"conv_filters\": 12, # Ensure \"conv_filters\" is divisible by \"heads\"\n",
    "\t\"kernel_size\": 3,\n",
    "\t\"hidden_dimensions\": 128,\n",
    "\t\"layers\": 6,\n",
    "\t\"heads\": 4,\n",
    "\t\"dropout_rate\": 0.05,\n",
    "\t\"is_batch_normalised\": True\n",
    "}\n",
    "train_and_crossvalidate_model(model_specification_1, folds=2, epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
