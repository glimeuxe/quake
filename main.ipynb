{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import DataPreprocessing, SpectrogramDataset, plot_waveform, plot_spectrogram\n",
    "\n",
    "ROOT_PATH = os.getcwd()\n",
    "\n",
    "RAW_DATASET_PATH = os.path.join(ROOT_PATH, \"dataset\", \"raw\")\n",
    "PRODUCTION_DATASET_PATH = os.path.join(ROOT_PATH, \"dataset\", \"production\")\n",
    "LOGS_PATH = os.path.join(ROOT_PATH, \"logs\")\n",
    "MODELS_PATH = os.path.join(ROOT_PATH, \"models\")\n",
    "\n",
    "SUBSAMPLE_SIZE = 1000\n",
    "NOISE_RATIO = 0.5\n",
    "EARTHQUAKE_RATIO = 0.5\n",
    "\n",
    "def preprocess_data():\n",
    "\t# Compute preprocessed data\n",
    "\tdata_processor = DataPreprocessing(\n",
    "\t\tsubsample_size=SUBSAMPLE_SIZE,\n",
    "\t\traw_dataset_path=RAW_DATASET_PATH,\n",
    "\t\tlogs_path=LOGS_PATH,\n",
    "\t\tnoise_ratio=NOISE_RATIO,\n",
    "\t\tearthquake_ratio=EARTHQUAKE_RATIO\n",
    "\t)\n",
    "\n",
    "\t# Save preprocessed data\n",
    "\tsignals = data_processor.subsample_traces\n",
    "\tnp.save(os.path.join(PRODUCTION_DATASET_PATH, f\"signals_{SUBSAMPLE_SIZE}.npy\"), np.array(list(signals.values())))\n",
    "\n",
    "\tmetadata = data_processor.subsample_metadata\n",
    "\tmetadata.to_feather(os.path.join(PRODUCTION_DATASET_PATH, f\"metadata_{SUBSAMPLE_SIZE}.feather\"))\n",
    "\n",
    "\twaveform_images = data_processor.create_waveform_images()\n",
    "\tnp.save(os.path.join(PRODUCTION_DATASET_PATH, f\"waveform_images_{SUBSAMPLE_SIZE}.npy\"), waveform_images)\n",
    "\n",
    "\tspectrogram_images = data_processor.create_spectrogram_images()\n",
    "\tnp.save(os.path.join(PRODUCTION_DATASET_PATH, f\"spectrogram_images_{SUBSAMPLE_SIZE}.npy\"), spectrogram_images)\n",
    "\n",
    "\t# Print summary of preprocessed data\n",
    "\tprint(f\"\"\"\n",
    "\t1. Expected samples: {SUBSAMPLE_SIZE}\n",
    "\t2. Actual samples: {len(signals)}\\n{metadata['category'].value_counts(normalize=True)}\n",
    "\t\"\"\")\n",
    "\n",
    "preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_data(num_samples=1):\n",
    "\t\"\"\"Randomly selects and plots waveform and spectrogram images.\"\"\"\n",
    "\t# Load saved images\n",
    "\twaveform_images = np.load(os.path.join(PRODUCTION_DATASET_PATH, f\"waveform_images_{SUBSAMPLE_SIZE}.npy\"))\n",
    "\tspectrogram_images = np.load(os.path.join(PRODUCTION_DATASET_PATH, f\"spectrogram_images_{SUBSAMPLE_SIZE}.npy\"))\n",
    "\n",
    "\t# Create figure with subplots\n",
    "\tfig, axes = plt.subplots(num_samples, 2, figsize=(6, 3 * num_samples))\n",
    "\n",
    "\t# Ensure axes is iterable for a single sample case\n",
    "\tif num_samples == 1:\n",
    "\t\taxes = [axes]\n",
    "\n",
    "\t# Plot random images\n",
    "\tfor i1, i2 in enumerate(np.random.choice(len(waveform_images), num_samples, replace=False)):\n",
    "\t\taxes[i1][0].imshow(waveform_images[i2])\n",
    "\t\taxes[i1][0].set_title(f\"Waveform {i2}\")\n",
    "\t\taxes[i1][0].axis(\"off\")\n",
    "\t\taxes[i1][1].imshow(spectrogram_images[i2])\n",
    "\t\taxes[i1][1].set_title(f\"Spectrogram {i2}\")\n",
    "\t\taxes[i1][1].axis(\"off\")\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "visualise_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 37\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 50\n",
    "PATIENCE = 7\n",
    "\n",
    "# Load production dataset and compute labels\n",
    "spectrograms = np.load(os.path.join(PRODUCTION_DATASET_PATH, f\"spectrogram_images_{SUBSAMPLE_SIZE}.npy\"))\n",
    "metadata = pd.read_feather(os.path.join(PRODUCTION_DATASET_PATH, f\"metadata_{SUBSAMPLE_SIZE}.feather\"))\n",
    "labels = (metadata[\"category\"] == \"earthquake\").astype(np.float32).values  # 0 (noise), 1 (earthquake)\n",
    "\n",
    "# Normalize and convert spectrograms and labels to tensors\n",
    "spectrograms = torch.tensor(spectrograms.astype(np.float32) / 255.0).permute(0, 3, 1, 2)\n",
    "labels = torch.tensor(labels).view(-1, 1)\n",
    "\n",
    "def split_data(spectrograms, labels, test_size=0.2, dev_size=0.5):\n",
    "\ttrain_i, temp_i, train_labels, temp_labels = train_test_split(spectrograms, labels, test_size=test_size, stratify=labels, random_state=SEED)\n",
    "\tdev_i, test_i, dev_labels, test_labels = train_test_split(temp_i, temp_labels, test_size=dev_size, stratify=temp_labels, random_state=SEED)\n",
    "\treturn train_i, train_labels, dev_i, dev_labels, test_i, test_labels\n",
    "\n",
    "# Split production dataset into train, dev, and test sets\n",
    "train_i, train_labels, dev_i, dev_labels, test_i, test_labels = split_data(spectrograms, labels)\n",
    "\n",
    "# Convert train, dev, and test sets to train, dev, and test loaders\n",
    "train_loader = DataLoader(SpectrogramDataset(train_i, train_labels), batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(SpectrogramDataset(dev_i, dev_labels), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(SpectrogramDataset(test_i, test_labels), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def train_model(model, train_loader, dev_loader, criterion, optimizer):\n",
    "\t# Create model folder\n",
    "\tmodel_folder = os.path.join(MODELS_PATH, f\"{model.id}\")\n",
    "\tos.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "\tbest_dev_loss = float(\"inf\")\n",
    "\tbest_model_path = os.path.join(model_folder, \"best.pth\")\n",
    "\tpatience_counter = 0\n",
    "\ttrain_losses = []\n",
    "\tdev_losses = []\n",
    "\n",
    "\tfor epoch in range(NUM_EPOCHS):\n",
    "\t\t# Train model on train set\n",
    "\t\tmodel.train()\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor images, labels in train_loader:\n",
    "\t\t\timages, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutputs = model(images)\n",
    "\t\t\tloss = criterion(outputs, labels)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\ttrain_loss = total_loss / len(train_loader)\n",
    "\t\ttrain_losses.append(train_loss)\n",
    "\n",
    "\t\t# Evaluate model on dev set\n",
    "\t\tmodel.eval()\n",
    "\t\tdev_loss = 0\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor images, labels in dev_loader:\n",
    "\t\t\t\timages, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\t\t\t\toutputs = model(images)\n",
    "\t\t\t\tdev_loss += criterion(outputs, labels).item()\n",
    "\t\tdev_loss /= len(dev_loader)\n",
    "\t\tdev_losses.append(dev_loss)\n",
    "\n",
    "\t\tprint(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Train Loss: {train_loss:.4f} - Dev Loss: {dev_loss:.4f}\")\n",
    "\n",
    "\t\t# Save model epoch checkpoint\n",
    "\t\ttorch.save(model.state_dict(), os.path.join(model_folder, f\"epoch={epoch+1}-dev_loss={dev_loss:.4f}.pth\"))\n",
    "\n",
    "\t\tif dev_loss < best_dev_loss:\n",
    "\t\t\tbest_dev_loss = dev_loss\n",
    "\n",
    "\t\t\t# Save best model\n",
    "\t\t\ttorch.save(model.state_dict(), best_model_path)\n",
    "\t\t\tpatience_counter = 0  # Reset patience counter\n",
    "\t\telse:\n",
    "\t\t\tpatience_counter += 1\n",
    "\t\t\t# Stop early if needed\n",
    "\t\t\tif patience_counter >= PATIENCE:\n",
    "\t\t\t\tprint(\"Stopping early\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t# Save losses\n",
    "\tloss_data = {\"train_losses\": train_losses, \"dev_losses\": dev_losses}\n",
    "\twith open(os.path.join(model_folder, \"losses.json\"), \"w\") as f:\n",
    "\t\tjson.dump(loss_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(CNN1, self).__init__()\n",
    "\t\tself.id = \"CNN1\"\n",
    "\t\tself.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "\t\tself.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "\t\tself.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\t\tself.dropout = nn.Dropout(0.5)\n",
    "\t\tself.fc1 = nn.Linear(64 * 25 * 37, 128)\n",
    "\t\tself.fc2 = nn.Linear(128, 16)\n",
    "\t\tself.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.pool(F.relu(self.conv1(x)))\n",
    "\t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "\t\tx = self.pool(F.relu(self.conv3(x)))\n",
    "\t\tx = torch.flatten(x, 1)\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = self.dropout(x)\n",
    "\t\tx = F.relu(self.fc2(x))\n",
    "\t\treturn torch.sigmoid(self.fc3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierFeatureMapping(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels):\n",
    "\t\tsuper(FourierFeatureMapping, self).__init__()\n",
    "\t\tself.W = nn.Parameter(torch.randn(in_channels, out_channels) * 10, requires_grad=False)\n",
    "\t\tself.b = nn.Parameter(torch.rand(out_channels) * 2 * torch.pi, requires_grad=False)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx_proj = torch.einsum('bchw,cd->bdhw', x, self.W) + self.b[None, :, None, None]\n",
    "\t\treturn torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=1)\n",
    "\n",
    "class CNNFFMe(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(CNNFFMe, self).__init__()\n",
    "\t\tself.id = \"CNNFFMe\"\n",
    "\t\tself.fourier = FourierFeatureMapping(3, 6)  # Expand input to 6 channels\n",
    "\t\tself.conv1 = nn.Conv2d(12, 16, kernel_size=3, padding=1)\n",
    "\t\tself.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "\t\tself.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\t\tself.dropout = nn.Dropout(0.5)\n",
    "\t\tself.fc1 = nn.Linear(64 * 25 * 37, 128)\n",
    "\t\tself.fc2 = nn.Linear(128, 16)\n",
    "\t\tself.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.fourier(x)  # Apply Fourier feature mapping\n",
    "\t\tx = self.pool(F.relu(self.conv1(x)))\n",
    "\t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "\t\tx = self.pool(F.relu(self.conv3(x)))\n",
    "\t\tx = torch.flatten(x, 1)\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = self.dropout(x)\n",
    "\t\tx = F.relu(self.fc2(x))\n",
    "\t\treturn torch.sigmoid(self.fc3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_KolArn(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(CNN_KolArn, self).__init__()\n",
    "\t\tself.id = \"CNN_KolArn\"\n",
    "\n",
    "\t\tself.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "\t\tself.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "\t\tself.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\t\tself.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.dropout = nn.Dropout(0.5)\n",
    "\t\tself.fc1 = nn.Linear(64 * 25 * 37, 128)\n",
    "\t\tself.kolarn1 = self.kol_arn_layer(128, 128)\n",
    "\t\tself.fc2 = nn.Linear(128, 16)\n",
    "\t\tself.kolarn2 = self.kol_arn_layer(16, 16)\n",
    "\t\tself.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "\tdef kol_arn_layer(self, in_features, out_features):\n",
    "\t\t\"\"\"Kolmogorov-Arnold layer: Expands features non-linearly, then projects down.\"\"\"\n",
    "\t\thidden = in_features * 2  # Expand feature space\n",
    "\t\treturn nn.Sequential(\n",
    "\t\t\tnn.Linear(in_features, hidden),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(hidden, out_features)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.pool(F.relu(self.conv1(x)))\n",
    "\t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "\t\tx = self.pool(F.relu(self.conv3(x)))\n",
    "\t\tx = torch.flatten(x, 1)\n",
    "\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = self.kolarn1(x)\n",
    "\t\tx = self.dropout(x)\n",
    "\n",
    "\t\tx = F.relu(self.fc2(x))\n",
    "\t\tx = self.kolarn2(x)\n",
    "\n",
    "\t\treturn torch.sigmoid(self.fc3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_KolArnSC(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(CNN_KolArnSC, self).__init__()\n",
    "\t\tself.id = \"CNN_KolArnSC\"\n",
    "\n",
    "\t\t# Convolutional layers\n",
    "\t\tself.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "\t\tself.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "\t\tself.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\n",
    "\t\t# Pooling layer\n",
    "\t\tself.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "\t\t# Kolmogorov-Arnold Skip Connections (Now channel-wise using 1x1 convolutions)\n",
    "\t\tself.kolarn_skip1 = self.kol_arn_layer(16, 16)\n",
    "\t\tself.kolarn_skip2 = self.kol_arn_layer(32, 32)\n",
    "\n",
    "\t\t# Dropout layer\n",
    "\t\tself.dropout = nn.Dropout(0.5)\n",
    "\n",
    "\t\t# Fully connected layers with Kolmogorov-Arnold units\n",
    "\t\tself.fc1 = nn.Linear(64 * 25 * 37, 128)\n",
    "\t\tself.kolarn1 = self.kol_arn_layer(128, 128, fc=True)\n",
    "\t\tself.fc2 = nn.Linear(128, 16)\n",
    "\t\tself.kolarn2 = self.kol_arn_layer(16, 16, fc=True)\n",
    "\t\tself.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "\tdef kol_arn_layer(self, in_channels, out_channels, fc=False):\n",
    "\t\t\"\"\"Kolmogorov-Arnold layer: Expands features non-linearly, then projects down.\n",
    "\t\t   Uses 1x1 convs for spatial features, FC for dense layers.\n",
    "\t\t\"\"\"\n",
    "\t\tif fc:\n",
    "\t\t\thidden = in_channels * 2  # Expand feature space for FC layers\n",
    "\t\t\treturn nn.Sequential(\n",
    "\t\t\t\tnn.Linear(in_channels, hidden),\n",
    "\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\tnn.Linear(hidden, out_channels)\n",
    "\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\treturn nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(in_channels, in_channels * 2, kernel_size=1),  # Expand\n",
    "\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\tnn.Conv2d(in_channels * 2, out_channels, kernel_size=1)  # Contract\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# First convolution + skip connection\n",
    "\t\tx1 = self.pool(F.relu(self.conv1(x)))\n",
    "\t\tx1 = x1 + self.kolarn_skip1(x1)  # Kol-Arn skip connection\n",
    "\n",
    "\t\t# Second convolution + skip connection\n",
    "\t\tx2 = self.pool(F.relu(self.conv2(x1)))\n",
    "\t\tx2 = x2 + self.kolarn_skip2(x2)  # Kol-Arn skip connection\n",
    "\n",
    "\t\t# Third convolution\n",
    "\t\tx3 = self.pool(F.relu(self.conv3(x2)))\n",
    "\n",
    "\t\t# Flatten for fully connected layers\n",
    "\t\tx3 = torch.flatten(x3, 1)\n",
    "\n",
    "\t\t# Fully connected layers with Kolmogorov-Arnold transformations\n",
    "\t\tx3 = F.relu(self.fc1(x3))\n",
    "\t\tx3 = self.kolarn1(x3)\n",
    "\t\tx3 = self.dropout(x3)\n",
    "\n",
    "\t\tx3 = F.relu(self.fc2(x3))\n",
    "\t\tx3 = self.kolarn2(x3)\n",
    "\n",
    "\t\treturn torch.sigmoid(self.fc3(x3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KolArnLayer(nn.Module):\n",
    "\tdef __init__(self, in_features, out_features):\n",
    "\t\tsuper(KolArnLayer, self).__init__()\n",
    "\t\tself.fc1 = nn.Linear(in_features, out_features)\n",
    "\t\tself.fc2 = nn.Linear(out_features, out_features)\n",
    "\t\tself.activation = nn.Sigmoid()  # Can experiment with Tanh, Swish, etc.\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.activation(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\treturn x\n",
    "\n",
    "class GatedAttentionUnit(nn.Module):\n",
    "\t\"\"\"Gated Attention Unit (GAU) for enhanced feature selection.\"\"\"\n",
    "\tdef __init__(self, in_channels):\n",
    "\t\tsuper(GatedAttentionUnit, self).__init__()\n",
    "\t\tself.conv_f = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "\t\tself.conv_g = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "\t\tself.conv_h = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tf = self.conv_f(x)\n",
    "\t\tg = self.conv_g(x)\n",
    "\t\tattention = self.sigmoid(f * g)  # Element-wise multiplication\n",
    "\t\th = self.conv_h(x)\n",
    "\t\treturn attention * h + x  # Residual connection\n",
    "\n",
    "class CNN_KolArn_GAU(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(CNN_KolArn_GAU, self).__init__()\n",
    "\t\tself.id = \"CNN_KolArn_GAU\"\n",
    "\t\tself.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "\t\tself.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "\t\tself.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\t\tself.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.gau1 = GatedAttentionUnit(16)\n",
    "\t\tself.gau2 = GatedAttentionUnit(32)\n",
    "\t\tself.gau3 = GatedAttentionUnit(64)\n",
    "\t\tself.fc1 = nn.Linear(64 * 25 * 37, 128)\n",
    "\t\tself.kol_arn = KolArnLayer(128, 64)\n",
    "\t\tself.fc2 = nn.Linear(64, 16)\n",
    "\t\tself.fc3 = nn.Linear(16, 1)\n",
    "\t\tself.dropout = nn.Dropout(0.5)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.pool(F.relu(self.gau1(self.conv1(x))))\n",
    "\t\tx = self.pool(F.relu(self.gau2(self.conv2(x))))\n",
    "\t\tx = self.pool(F.relu(self.gau3(self.conv3(x))))\n",
    "\n",
    "\t\tx = torch.flatten(x, 1)\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = self.kol_arn(x)\n",
    "\t\tx = self.dropout(x)\n",
    "\t\tx = F.relu(self.fc2(x))\n",
    "\t\treturn torch.sigmoid(self.fc3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KolArnLayer(nn.Module):\n",
    "\tdef __init__(self, in_features, out_features):\n",
    "\t\tsuper(KolArnLayer, self).__init__()\n",
    "\t\tself.fc1 = nn.Linear(in_features, out_features)\n",
    "\t\tself.fc2 = nn.Linear(out_features, out_features)\n",
    "\t\tself.activation = nn.Sigmoid()  # Can be Tanh, Swish, etc.\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.activation(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\treturn x\n",
    "\n",
    "class CNN_KolArn_Cycle(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(CNN_KolArn_Cycle, self).__init__()\n",
    "\t\tself.id = \"CNN_KolArn_Cycle\"\n",
    "\t\tself.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "\t\tself.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "\t\tself.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\t\tself.pool = nn.MaxPool2d(2, 2)\n",
    "\t\tself.conv1_backflow = nn.Conv2d(16, 3, kernel_size=1)  # Cyclic backflow\n",
    "\t\tself.conv2_backflow = nn.Conv2d(32, 16, kernel_size=1)\n",
    "\t\tself.fc1 = nn.Linear(80 * 25 * 37, 128)\n",
    "\t\tself.kol_arn = KolArnLayer(128, 64)  # Kolmogorov-Arnold mapping\n",
    "\t\tself.fc2 = nn.Linear(64, 16)\n",
    "\t\tself.fc3 = nn.Linear(16, 1)\n",
    "\t\tself.dropout = nn.Dropout(0.5)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# Standard forward pass\n",
    "\t\tx1 = self.pool(F.relu(self.conv1(x)))  # 16 channels -> [batch, 16, 100, 150]\n",
    "\t\tx2 = self.pool(F.relu(self.conv2(x1)))  # 32 channels -> [batch, 32, 50, 75]\n",
    "\t\tx3 = self.pool(F.relu(self.conv3(x2)))  # 64 channels -> [batch, 64, 25, 37]\n",
    "\t\t# Cyclic backflow: Resize and inject feature maps\n",
    "\t\tx1_feedback = F.relu(self.conv1_backflow(x1))  # [batch, 3, 100, 150]\n",
    "\t\tx1_feedback = F.interpolate(x1_feedback, size=(200, 300), mode=\"bilinear\", align_corners=False)\n",
    "\t\tx2_feedback = F.relu(self.conv2_backflow(x2))  # [batch, 16, 50, 75]\n",
    "\t\tx2_feedback = F.interpolate(x2_feedback, size=(100, 150), mode=\"bilinear\", align_corners=False)\n",
    "\t\tx1_refined = x + x1_feedback  # Inject x1_feedback back into input\n",
    "\t\tx2_refined = x1 + x2_feedback  # Inject x2_feedback back into x1\n",
    "\t\tx2_refined = F.interpolate(x2_refined, size=(25, 37), mode=\"bilinear\", align_corners=False)\n",
    "\t\tx_concat = torch.cat([x3, x2_refined], dim=1)\n",
    "\t\tx = torch.flatten(x_concat, 1)\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = self.kol_arn(x)\n",
    "\t\tx = self.dropout(x)\n",
    "\t\tx = F.relu(self.fc2(x))\n",
    "\t\treturn torch.sigmoid(self.fc3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_KolArnSC().to(DEVICE)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_model(model, train_loader, dev_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "\ty_true, y_pred = y_true.cpu().numpy(), (y_pred.cpu().numpy() > 0.5)\n",
    "\tacc = accuracy_score(y_true, y_pred)\n",
    "\tprecision = precision_score(y_true, y_pred, zero_division=0)\n",
    "\trecall = recall_score(y_true, y_pred)\n",
    "\ttn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\ttpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\tfpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\treturn {\n",
    "\t\t\"accuracy\": round(acc, 6),\n",
    "\t\t\"precision\": round(precision, 6),\n",
    "\t\t\"TPR\": float(round(tpr, 6)),\n",
    "\t\t\"FPR\": float(round(fpr, 6))\n",
    "\t}\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "\tmodel.eval()\n",
    "\ty_true, y_pred = [], []\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor images, labels in test_loader:\n",
    "\t\t\timages, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\t\t\toutputs = model(images)\n",
    "\t\t\ty_true.append(labels)\n",
    "\t\t\ty_pred.append(outputs)\n",
    "\n",
    "\ty_true = torch.cat(y_true, dim=0)\n",
    "\ty_pred = torch.cat(y_pred, dim=0)\n",
    "\tmetrics = compute_metrics(y_true, y_pred)\n",
    "\tprint(f\"Test Metrics: {metrics}\")\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(MODELS_PATH, f\"{model.id}\", \"best.pth\")))\n",
    "evaluate_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
